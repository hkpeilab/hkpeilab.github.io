<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.3 for Hugo"><meta name=description content="Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays."><link rel=alternate hreflang=en-us href=https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload="this.media='all'"><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/css/wowchemy.14ed061da97974d1963d99308520201c.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hu120395325dac92a7b24d7fd791d5bf28_509340_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu120395325dac92a7b24d7fd791d5bf28_509340_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="PEILab"><meta property="og:url" content="https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/"><meta property="og:title" content="Warming Serverless ML Inference via Inter-function Model Transformation | PEILab"><meta property="og:description" content="Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays."><meta property="og:image" content="https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/featured.png"><meta property="twitter:image" content="https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-09-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-12T22:33:58+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/"},"headline":"Warming Serverless ML Inference via Inter-function Model Transformation","image":["https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/featured.png"],"datePublished":"2023-09-01T00:00:00Z","dateModified":"2023-09-12T22:33:58+08:00","author":{"@type":"Person","name":"Authors"},"publisher":{"@type":"Organization","name":"PEILab","logo":{"@type":"ImageObject","url":"https://peilab.netlify.app/media/logo_hude7c550c9b1c4d0119cb1e6619a815ae_62586_192x192_fit_lanczos_2.png"}},"description":"Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays."}</script><title>Warming Serverless ML Inference via Inter-function Model Transformation | PEILab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=8b70d63187ad914b79169352492f08f2><script src=/js/wowchemy-init.min.b024895df05e271929739bb54886c674.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/media/logo_hude7c550c9b1c4d0119cb1e6619a815ae_62586_0x70_resize_lanczos_2.png alt=PEILab></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/media/logo_hude7c550c9b1c4d0119cb1e6619a815ae_62586_0x70_resize_lanczos_2.png alt=PEILab></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/research><span>Research</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/demos><span>Demos</span></a></li><li class=nav-item><a class=nav-link href=/post><span>News</span></a></li><li class=nav-item><a class=nav-link href=/workshop><span>Workshop</span></a></li><li class=nav-item><a class=nav-link href=/gallery><span>Gallery</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><article class="article article-project"><div class="article-container pt-3"><h1>Warming Serverless ML Inference via Inter-function Model Transformation</h1><div class=article-metadata><span class=article-date>Last updated on
Sep 12, 2023</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:568px><div style=position:relative><img src=/project/warming-serverless-ml-inference-via-inter-function-model-transformation/featured_hu25ba95c0b41d300c1e414c7f0b75c04e_229005_720x0_resize_lanczos_2.png alt class=featured-image></div></div><div class=article-container><div class=article-style><p>Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays. Unfortunately, most existing works do not work well because they still need to load models into containers from scratch, which is the bottleneck based on our observations. Therefore, we propose a low-latency serverless ML inference system called Optimus via a new container management scheme.</p><h2 id=prototype-overview>Prototype Overview</h2><p>Optimus API and communication between clients and the gateway are implemented in REST API format. Clients can invoke an inference procedure by sending an HTTP request containing the model name and input data. Optimus stores the trained models in a Docker volume that is attached directly to each container created by the system. Models are deployed to the Docker volume in HDF format. Model structure information and model-to-model transformation planing are stored with the models in JSON format. On the host machine, a gateway service runs as the container manager. It uses the Docker SDK for Python to create, run, and remove containers in the local Docker environment. It also runs a Flask HTTP server that accepts client requests and sends them to containers.</p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_50ba05d0065e2b88ccccbb7992813146.png 400w,
/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_14507c1104a9bbccb276982a2d185795.png 760w,
/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_1200x1200_fit_lanczos_2.png 1200w" src=/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_50ba05d0065e2b88ccccbb7992813146.png width=760 height=456 loading=lazy data-zoomable></div></div></figure></div><div class=article-tags><a class="badge badge-light" href=/tags/ai-computing-cyberinfrastructure/>AI Computing Cyberinfrastructure</a></div><div class="project-related-pages content-widget-hr"></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Copyright © The Pervasive Edge Intelligence Laboratory Reserved. 2023</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/_vendor/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.e844b7881168826667ba422c36255a69.js></script><script async defer src=https://buttons.github.io/buttons.js></script></body></html>