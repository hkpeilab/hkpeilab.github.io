<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Qianli Liu | PEILab</title><link>https://hkpeilab.netlify.app/people/qianli-liu/</link><atom:link href="https://hkpeilab.netlify.app/people/qianli-liu/index.xml" rel="self" type="application/rss+xml"/><description>Qianli Liu</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2025</copyright><image><url>https://hkpeilab.netlify.app/people/qianli-liu/avatar_huadfe1256184023cf0fcf60ae2a93817a_113302_270x270_fill_q100_lanczos_center.jpg</url><title>Qianli Liu</title><link>https://hkpeilab.netlify.app/people/qianli-liu/</link></image><item><title>Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management</title><link>https://hkpeilab.netlify.app/publications/mell-memory-efficient-large-language-model-serving-via-multi-gpu-kv-cache-management/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/publications/mell-memory-efficient-large-language-model-serving-via-multi-gpu-kv-cache-management/</guid><description/></item></channel></rss>