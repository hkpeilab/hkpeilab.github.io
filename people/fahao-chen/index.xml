<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fahao Chen | PEILab</title><link>https://hkpeilab.netlify.app/people/fahao-chen/</link><atom:link href="https://hkpeilab.netlify.app/people/fahao-chen/index.xml" rel="self" type="application/rss+xml"/><description>Fahao Chen</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2025</copyright><lastBuildDate>Tue, 20 May 2025 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu327f33d11c6af42c6fedb6aa301ebf3b_346233_300x300_fit_lanczos_2.png</url><title>Fahao Chen</title><link>https://hkpeilab.netlify.app/people/fahao-chen/</link></image><item><title>Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management</title><link>https://hkpeilab.netlify.app/publications/mell-memory-efficient-large-language-model-serving-via-multi-gpu-kv-cache-management/</link><pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/publications/mell-memory-efficient-large-language-model-serving-via-multi-gpu-kv-cache-management/</guid><description/></item><item><title>Hare: Exploiting Inter-job and Intra-job Parallelism of Distributed Machine Learning on Heterogeneous GPUs</title><link>https://hkpeilab.netlify.app/publications/hare-exploiting-inter-job-and-intra-job-parallelism-of-distributed-machine-learning-on-heterogeneous-gpus/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/publications/hare-exploiting-inter-job-and-intra-job-parallelism-of-distributed-machine-learning-on-heterogeneous-gpus/</guid><description/></item></channel></rss>