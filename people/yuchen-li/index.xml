<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Yuchen Li | PEILab</title><link>https://hkpeilab.netlify.app/people/yuchen-li/</link><atom:link href="https://hkpeilab.netlify.app/people/yuchen-li/index.xml" rel="self" type="application/rss+xml"/><description>Yuchen Li</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu9de44823e73c1111f923ff26b68a2483_254409_300x300_fit_lanczos_2.png</url><title>Yuchen Li</title><link>https://hkpeilab.netlify.app/people/yuchen-li/</link></image><item><title>Throughput Maximization of Delay-Aware DNN Inference in Edge Computing by Exploring DNN Model Partitioning and Inference Parallelism</title><link>https://hkpeilab.netlify.app/publications/throughput-maximization-of-delay-aware-dnn-inference-in-edge-computing-by-exploring-dnn-model-partitioning-and-inference-parallelism/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/publications/throughput-maximization-of-delay-aware-dnn-inference-in-edge-computing-by-exploring-dnn-model-partitioning-and-inference-parallelism/</guid><description/></item></channel></rss>