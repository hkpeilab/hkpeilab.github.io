<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Computing Cyberinfrastructure | PEILab</title><link>https://peilab.netlify.app/tags/ai-computing-cyberinfrastructure/</link><atom:link href="https://peilab.netlify.app/tags/ai-computing-cyberinfrastructure/index.xml" rel="self" type="application/rss+xml"/><description>AI Computing Cyberinfrastructure</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2023</copyright><lastBuildDate>Fri, 01 Sep 2023 00:00:00 +0000</lastBuildDate><image><url>https://peilab.netlify.app/media/logo_hude7c550c9b1c4d0119cb1e6619a815ae_62586_300x300_fit_lanczos_2.png</url><title>AI Computing Cyberinfrastructure</title><link>https://peilab.netlify.app/tags/ai-computing-cyberinfrastructure/</link></image><item><title>An Elastic Transformer Serving System for Foundation Model via Token Adaptation</title><link>https://peilab.netlify.app/project/an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://peilab.netlify.app/project/an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/</guid><description>&lt;p>Transformer model empowered architectures have become a pillar of cloud services that keeps reshaping our society. However, the dynamic query loads and heterogeneous user requirements severely challenge current transformer serving systems, which rely on pre-training multiple variants of a foundation model to accommodate varying service demands. Unfortunately, such a mechanism is unsuitable for large transformer models due to the prohibitive training costs and excessive I/O delay. We introduce OTAS, the first elastic serving system specially tailored for transformer models by exploring lightweight token management. We develop a novel idea called token adaptation. To cope with fluctuating query loads and diverse user requests, we enhance OTAS with application-aware selective batching and online token adaptation.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>We use Python to process the incoming queries and PyTorch to define the neural networks. We build the transformer model with timm library and insert two modules to add and remove the processing tokens at each layer. The system enables users to make a query and register tasks with two interfaces. The Make Query interface processes a query that comprises an image sample and various attributes. The Register Task interface saves the task parameters in the task model list and the corresponding latency and utility values in the task data list.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_4e1cf762b6d7ec989fbfc50b1c0ced7c.png 400w,
/project/an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_e732651146b62d746651e812971b9fc1.png 760w,
/project/an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_1200x1200_fit_lanczos_2.png 1200w"
src="https://peilab.netlify.app/project/an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_4e1cf762b6d7ec989fbfc50b1c0ced7c.png"
width="309"
height="260"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item><item><title>Intelligence-Endogenous Management Platform for Computing and Network Convergence</title><link>https://peilab.netlify.app/project/intelligence-endogenous-management-platform-for-computing-and-network-convergence/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://peilab.netlify.app/project/intelligence-endogenous-management-platform-for-computing-and-network-convergence/</guid><description>&lt;p>Massive emerging applications are driving demand for the ubiquitous deployment of computing power today. This trend not only spurs the recent popularity of the Computing and Network Convergence (CNC), but also introduces an urgent need for the intelligentization of a management platform to coordinate changing resources and tasks in the CNC. Therefore, we present the concept of an intelligence-endogenous management platform for CNCs called CNC brain based on artificial intelligence technologies. It aims at efficiently and automatically matching the supply and demand with high heterogeneity in a CNC via four key building blocks, i.e., perception, scheduling, adaptation, and governance, throughout the CNC&amp;rsquo;s life cycle. Their functionalities, goals, and challenges are presented.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>To examine the effectiveness of the proposed concept and framework, we also implement a prototype for the CNC brain based on a deep reinforcement learning technology. Also, it is evaluated on a CNC testbed that integrates three open-source and popular frameworks (OpenFaas, Kubernetes, and Ethereum) and a real-world business dataset provided by Microsoft Azure. The evaluation results prove the prototype&amp;rsquo;s effectiveness in terms of resource utilization and performance.
Reference: &lt;a href="https://arxiv.org/abs/2308.03450">https://arxiv.org/abs/2308.03450&lt;/a>&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/intelligence-endogenous-management-platform-for-computing-and-network-convergence/1_hu2f3450ce12f4028d239f9eaf4bcb7d19_144802_ab7cdec498fa73f32f119eb06521f87d.png 400w,
/project/intelligence-endogenous-management-platform-for-computing-and-network-convergence/1_hu2f3450ce12f4028d239f9eaf4bcb7d19_144802_16b69f0ec855042302c46829ddfb8f56.png 760w,
/project/intelligence-endogenous-management-platform-for-computing-and-network-convergence/1_hu2f3450ce12f4028d239f9eaf4bcb7d19_144802_1200x1200_fit_lanczos_2.png 1200w"
src="https://peilab.netlify.app/project/intelligence-endogenous-management-platform-for-computing-and-network-convergence/1_hu2f3450ce12f4028d239f9eaf4bcb7d19_144802_ab7cdec498fa73f32f119eb06521f87d.png"
width="760"
height="346"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item><item><title>Warming Serverless ML Inference via Inter-function Model Transformation</title><link>https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/</guid><description>&lt;p>Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays. Unfortunately, most existing works do not work well because they still need to load models into containers from scratch, which is the bottleneck based on our observations. Therefore, we propose a low-latency serverless ML inference system called Optimus via a new container management scheme.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>Optimus API and communication between clients and the gateway are implemented in REST API format. Clients can invoke an inference procedure by sending an HTTP request containing the model name and input data. Optimus stores the trained models in a Docker volume that is attached directly to each container created by the system. Models are deployed to the Docker volume in HDF format. Model structure information and model-to-model transformation planing are stored with the models in JSON format. On the host machine, a gateway service runs as the container manager. It uses the Docker SDK for Python to create, run, and remove containers in the local Docker environment. It also runs a Flask HTTP server that accepts client requests and sends them to containers.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_50ba05d0065e2b88ccccbb7992813146.png 400w,
/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_14507c1104a9bbccb276982a2d185795.png 760w,
/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_1200x1200_fit_lanczos_2.png 1200w"
src="https://peilab.netlify.app/project/warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_50ba05d0065e2b88ccccbb7992813146.png"
width="760"
height="456"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item></channel></rss>