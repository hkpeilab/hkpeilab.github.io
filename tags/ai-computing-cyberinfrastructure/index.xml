<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Computing Cyberinfrastructure | PEILab</title><link>https://hkpeilab.netlify.app/tags/ai-computing-cyberinfrastructure/</link><atom:link href="https://hkpeilab.netlify.app/tags/ai-computing-cyberinfrastructure/index.xml" rel="self" type="application/rss+xml"/><description>AI Computing Cyberinfrastructure</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Wed, 20 Mar 2030 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu9de44823e73c1111f923ff26b68a2483_254409_300x300_fit_lanczos_2.png</url><title>AI Computing Cyberinfrastructure</title><link>https://hkpeilab.netlify.app/tags/ai-computing-cyberinfrastructure/</link></image><item><title>Introduction of "AI Computing Cyberinfrastructure"</title><link>https://hkpeilab.netlify.app/project/intro-ai-cyber/</link><pubDate>Wed, 20 Mar 2030 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/intro-ai-cyber/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;p>The unprecedented impact of foundation model technology, represented by ChatGPT, is driving a revolutionary paradigm shift in AI, bringing new opportunities and challenges to many industries. However, the high training, inference and maintenance costs of foundation model technologies limit their widespread adoption. Considering that edge computing power (e.g., edge servers, personal PCs, etc.), which accounts for about 90% of the network, has not yet been effectively used, we focus on a new federated edge AI cyberinfrastructure, and leverages the natural advantages of edge computing power in terms of cost, latency, and privacy, so as to efficiently aggregate the distributed, heterogeneous, and multi-party computing power and become an important computing power supplier in the computing power network, and promote the foundation model technology-based intelligent upgrading of various industries. And the infrastructure could be divided into several modules as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Elastic Foundation Model Inference&lt;/strong>: Elastic Foundation Model Inference is a dynamic approach designed to optimize the deployment and execution of foundation models. It leverages elastic computing resources to scale up or down based on the current demand and computational requirements. This flexibility ensures that FMs can handle varying workloads efficiently, providing robust performance across different tasks. &lt;a href="https://hkpeilab.netlify.app/project/infocom_an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/" target="_blank" rel="noopener">Details&lt;/a>&lt;/li>
&lt;li>&lt;strong>Fast Foundation Model Function Startup&lt;/strong>: Fast Foundation Model Function Startup refers to the rapid initialization and deployment of foundational AI models to ensure minimal latency and quick responsiveness in various applications. This approach is crucial for real-time applications and scenarios where time efficiency is paramount. &lt;a href="https://hkpeilab.netlify.app/project/eurosys_warming-serverless-ml-inference-via-inter-function-model-transformation/" target="_blank" rel="noopener">Details&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>(EuroSys2024) Warming Serverless ML Inference via Inter-function Model Transformation</title><link>https://hkpeilab.netlify.app/project/eurosys_warming-serverless-ml-inference-via-inter-function-model-transformation/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/eurosys_warming-serverless-ml-inference-via-inter-function-model-transformation/</guid><description>&lt;p>Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays. Unfortunately, most existing works do not work well because they still need to load models into containers from scratch, which is the bottleneck based on our observations. Therefore, we propose a low-latency serverless ML inference system called Optimus via a new container management scheme.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>Optimus API and communication between clients and the gateway are implemented in REST API format. Clients can invoke an inference procedure by sending an HTTP request containing the model name and input data. Optimus stores the trained models in a Docker volume that is attached directly to each container created by the system. Models are deployed to the Docker volume in HDF format. Model structure information and model-to-model transformation planing are stored with the models in JSON format. On the host machine, a gateway service runs as the container manager. It uses the Docker SDK for Python to create, run, and remove containers in the local Docker environment. It also runs a Flask HTTP server that accepts client requests and sends them to containers.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/eurosys_warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_50ba05d0065e2b88ccccbb7992813146.png 400w,
/project/eurosys_warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_14507c1104a9bbccb276982a2d185795.png 760w,
/project/eurosys_warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/eurosys_warming-serverless-ml-inference-via-inter-function-model-transformation/1_hue40c301b3c5d6105da1815942e48571c_182775_50ba05d0065e2b88ccccbb7992813146.png"
width="760"
height="456"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item><item><title>(INFOCOM2024) An Elastic Transformer Serving System for Foundation Model via Token Adaptation</title><link>https://hkpeilab.netlify.app/project/infocom_an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/infocom_an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/</guid><description>&lt;p>Transformer model empowered architectures have become a pillar of cloud services that keeps reshaping our society. However, the dynamic query loads and heterogeneous user requirements severely challenge current transformer serving systems, which rely on pre-training multiple variants of a foundation model to accommodate varying service demands. Unfortunately, such a mechanism is unsuitable for large transformer models due to the prohibitive training costs and excessive I/O delay. We introduce OTAS, the first elastic serving system specially tailored for transformer models by exploring lightweight token management. We develop a novel idea called token adaptation. To cope with fluctuating query loads and diverse user requests, we enhance OTAS with application-aware selective batching and online token adaptation.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>We use Python to process the incoming queries and PyTorch to define the neural networks. We build the transformer model with timm library and insert two modules to add and remove the processing tokens at each layer. The system enables users to make a query and register tasks with two interfaces. The Make Query interface processes a query that comprises an image sample and various attributes. The Register Task interface saves the task parameters in the task model list and the corresponding latency and utility values in the task data list.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/infocom_an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_4e1cf762b6d7ec989fbfc50b1c0ced7c.png 400w,
/project/infocom_an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_e732651146b62d746651e812971b9fc1.png 760w,
/project/infocom_an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/infocom_an-elastic-transformer-serving-system-for-foundation-model-via-token-adaptation-%E5%89%AF%E6%9C%AC/1_hu79deb2fad71fbde07430773b3e61f36e_56060_4e1cf762b6d7ec989fbfc50b1c0ced7c.png"
width="309"
height="260"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item></channel></rss>