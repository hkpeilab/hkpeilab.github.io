<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI for Science | PEILab</title><link>https://hkpeilab.netlify.app/tags/ai-for-science/</link><atom:link href="https://hkpeilab.netlify.app/tags/ai-for-science/index.xml" rel="self" type="application/rss+xml"/><description>AI for Science</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Wed, 20 Mar 2030 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu327f33d11c6af42c6fedb6aa301ebf3b_346233_300x300_fit_lanczos_2.png</url><title>AI for Science</title><link>https://hkpeilab.netlify.app/tags/ai-for-science/</link></image><item><title>Introduction of "AI for Science"</title><link>https://hkpeilab.netlify.app/project/ai-for-science/</link><pubDate>Wed, 20 Mar 2030 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/ai-for-science/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;p>AI for Science (AI4Science) is an emerging field that explores the intersection of artificial intelligence (AI) and scientific research. It leverages the power of AI techniques and algorithms to analyze vast amounts of scientific data, accelerate discovery, and enhance our understanding of complex scientific phenomena. AI4Science encompasses various domains, including chemistry, biology, physics, astronomy, and more, offering transformative opportunities for researchers and scientists. By harnessing machine learning, deep learning, and other AI methodologies, AI4Science enables data-driven insights, predictive modeling, and optimization in scientific investigations. It has the potential to revolutionize how we approach scientific research, making it more efficient, collaborative, and impactful. With its ability to uncover patterns, generate hypotheses, and assist in decision-making, AI4Science holds immense promise in driving breakthroughs, addressing research challenges, and opening up new frontiers in scientific exploration.&lt;/p></description></item><item><title>(arXiv2024) CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer</title><link>https://hkpeilab.netlify.app/project/cra5/</link><pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/cra5/</guid><description>&lt;p>The paper introduces CRA5, a project that uses the Variational Autoencoder Transformer (VAEformer) to compress the ERA5 climate dataset from 226TB to just 0.7TB, achieving a compression ratio of over 300 times. This extreme compression significantly reduces storage costs while preserving the dataset&amp;rsquo;s utility for accurate scientific analysis.&lt;/p>
&lt;p>This figure illustrates the comparison between the original ERA5 dataset and the compressed CRA5 dataset, showcasing the dramatic reduction in size. The VAEformer employs a low-complexity Auto-Encoder transformer that generates a quantized latent representation through variance inference, modeling the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding, outperforming existing compression methods.&lt;/p>
&lt;p>&lt;img src="1.png" alt="">
In this figure, the architecture of the VAEformer is depicted, highlighting its dual transformer-based variational auto-encoder structure. The below figure further details the Atmospheric Circulation Transformer Block, which utilizes different shapes of window attention to simulate various atmospheric circulation patterns. This innovative approach enables efficient data compression while maintaining high numerical accuracy, allowing for training of global weather forecasting models with performance comparable to using the full ERA5 dataset.&lt;/p>
&lt;p>&lt;img src="2.png" alt="">&lt;/p></description></item><item><title>(arXiv2024) WEATHER-5K: A Large-scale Global Station Weather Dataset Towards Comprehensive Time-series Forecasting Benchmark</title><link>https://hkpeilab.netlify.app/project/weather5k/</link><pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/weather5k/</guid><description>&lt;p>The paper presents the WEATHER-5K dataset, a comprehensive global weather station dataset designed to advance time-series weather forecasting benchmarks. WEATHER-5K includes data from 5,672 weather stations worldwide, covering a 10-year period with hourly intervals. It features multiple crucial weather variables, making it a valuable resource for evaluating and improving forecasting models.&lt;/p>
&lt;p>The dataset addresses limitations in existing public datasets, such as small size and limited temporal coverage, by providing a diverse and extensive collection of weather data. This allows for a more accurate reflection of real-world conditions and supports the development of models capable of generalizing across various regions and timeframes.&lt;/p>
&lt;p>Additionally, the dataset facilitates research beyond traditional methods, encouraging exploration in deep learning and data-driven approaches to improve forecast accuracy. Overall, WEATHER-5K serves as a robust foundation for advancing both operational weather services and scientific research in time-series forecasting.&lt;/p>
&lt;p>&lt;img src="1.png" alt="">&lt;/p></description></item><item><title>(arXiv2024) FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting</title><link>https://hkpeilab.netlify.app/project/fengwu-ghr/</link><pubDate>Sun, 28 Jan 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/fengwu-ghr/</guid><description>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="pipeline.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting The first data-driven global weather forecasting model running at the 0.09◦ horizontal resolution. FengWu-GHR introduces a novel approach that opens the door for operating ML-based high-resolution forecasts by inheriting prior knowledge from a pretrained low-resolution model. The hindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to the IFS-HRES. Furthermore, evaluations on station observations and case studies of extreme events support the competitive operational forecasting skill of FengWu-GHR at the high resolution.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>Kilometer-scale modeling of global atmosphere dynamics enables fine-grained weather forecasting and decreases the risk of disastrous weather and climate activity. Therefore, building a kilometer-scale global forecast model is a persistent pursuit in the meteorology domain. Active international efforts have been made in past decades to improve the spatial resolution of numerical weather models. Nonetheless, developing the higher resolution numerical model remains a long-standing challenge due to the substantial consumption of computational resources. Recent advances in data-driven global weather forecasting models utilize reanalysis data for model training and have demonstrated comparable or even higher forecasting skills than numerical models. However, they are all limited by the resolution of reanalysis data and incapable of generating higherresolution forecasts. This work presents FengWu-GHR, the first data-driven global weather forecasting model running at the 0.09◦ horizontal resolution. FengWu-GHR introduces a novel approach that opens the door for operating MLbased high-resolution forecasts by inheriting prior knowledge from a pretrained low-resolution model. The hindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to the IFS-HRES. Furthermore, evaluations on station observations and case studies of extreme events support the competitive operational forecasting skill of FengWu-GHR at the high resolution.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/fengwu-ghr/1_hu281c7a8ece870de5c49490bd79e1063b_419845_d9be2a2e1059a7f057aaabdd2212521e.png 400w,
/project/fengwu-ghr/1_hu281c7a8ece870de5c49490bd79e1063b_419845_4e995e45ecba54e1b762082c496a56e4.png 760w,
/project/fengwu-ghr/1_hu281c7a8ece870de5c49490bd79e1063b_419845_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/fengwu-ghr/1_hu281c7a8ece870de5c49490bd79e1063b_419845_d9be2a2e1059a7f057aaabdd2212521e.png"
width="509"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item></channel></rss>