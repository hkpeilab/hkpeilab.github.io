<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>None | PEILab</title><link>https://hkpeilab.netlify.app/tags/none/</link><atom:link href="https://hkpeilab.netlify.app/tags/none/index.xml" rel="self" type="application/rss+xml"/><description>None</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Wed, 20 Apr 2022 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu9de44823e73c1111f923ff26b68a2483_254409_300x300_fit_lanczos_2.png</url><title>None</title><link>https://hkpeilab.netlify.app/tags/none/</link></image><item><title>(IoTJ)A Unified TinyML System for Multi-modal Edge Intelligence and Real-time Visual Perception.</title><link>https://hkpeilab.netlify.app/project/unified-tinyml-system-main/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/unified-tinyml-system-main/</guid><description>&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Modern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks and saving resource costs. To conquer the limitations of conventional in-cloud computing, it comes the rise of on-device learning, which handles the end-to-end ML procedure mainly on user devices, and restricts unnecessary involvement of the cloud. Despite the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces many severe challenges, such as insufficient user training data, backward propagation blocking and limited peak processing speed.&lt;/p>
&lt;p>&lt;strong>Illustration:&lt;/strong> Conventional ML applications rely on the in-cloud learning paradigm, incurring essential drawbacks. Upgrading to the TinyML paradigm can effectively address these issues.&lt;/p>
&lt;h2 id="2-architecture-overview">2. Architecture Overview&lt;/h2>
&lt;p>Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, our group devote to designing high-performance TinyML architectures and relevant optimization algorithms, especially for embedded devices and microprocessors. Our research focuses on the software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization and hardware-level instruction acceleration. Here, we present the architecture overview of our system design.&lt;/p>
&lt;figure id="figure-architecture-overview">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Architecture Overview" srcset="
/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_1893169e646002294db55a3bcbb8d652.png 400w,
/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_924c49efc81088ccd881b8efbd832dbc.png 760w,
/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_1893169e646002294db55a3bcbb8d652.png"
width="760"
height="388"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Architecture Overview
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Illustration:&lt;/strong> an efficient TinyML system require a holistic design of the entire hierarchy, which can be resolved as five key research opportunities.&lt;/p>
&lt;h2 id="3-research-opportunities">3. Research Opportunities&lt;/h2>
&lt;p>Here are five key research opportunities to implement our system. Please check the sub-folders for details.&lt;/p>
&lt;h2 id="4-achievements">4. Achievements&lt;/h2>
&lt;p>The on-device learning techniques can be employed in many emerging TinyML scenarios, where the system performance is often bounded by the limited hardware resources. Currently, our group has achieved breakthroughs in improving the computational capacity and designing domain-specific AI chips for task acceleration. These chips can be designed from the perspectives of model compression, few-shot learning, quantization-ware training, memory management and low-level instructions. We pursue the vision that helps researchers and developers optimize AI deployment without tedious code modifications. Some research demos have been open-source on Github, please visit at:&lt;/p>
&lt;ul>
&lt;li>
&lt;i class="fab fa-github-square pr-1 fa-fw">&lt;/i>&lt;a href="https://github.com/kimihe" target="_blank" rel="noopener">https://github.com/kimihe&lt;/a>&lt;/li>
&lt;li>
&lt;i class="fab fa-github-square pr-1 fa-fw">&lt;/i>&lt;a href="https://github.com/FromSystem" target="_blank" rel="noopener">https://github.com/FromSystem&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="5-related-publications">5. Related Publications&lt;/h2>
&lt;p>[1] Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning, In Proc. of USENIX Annual Technical Conference (ATC), 2021 (CCF-A).
[2] On-device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective, IEEE Internet of Things Journal, 2020 (JCR-Q1).
[3] Petrel: Heterogeneity-aware Distributed Deep Learning via Hybrid Synchronization, IEEE Transactions on Parallel and Distributed Systems (TPDS), 2020 (CCF-A).
[4] Dual-view Attention Networks for Single Image Super-Resolution, In Proc. of the ACM International Conference on Multimedia (MM), 2020 (CCF-A).&lt;/p>
&lt;h2 id="6cooperators">6. Cooperators&lt;/h2>
&lt;p>Our group have established close cooperation with industrial communities, including Microsoft Research Asia, Alibaba DAMO Academy, Huawei Cloud, etc.&lt;/p>
&lt;h2 id="7phdintern-applications">7. PhD/intern Applications:&lt;/h2>
&lt;p>We are looking for students and partners who are interested in:&lt;/p>
&lt;p>(1) On-device/TinyML Systems (for Edge Intelligence)&lt;br>
(2) Distributed Machine Learning Systems (for Data center)&lt;br>
(3) Modern AI/ML frameworks: e.g., NVIDIA NCCL, CUDA, TensorRT, Apple CoreML, PyTorch, TensorFlow, Keras, BytePS, Gym, etc.&lt;br>
(4) Domain-specific hardware optimization and implementation, e.g., NVIDIA Jetson, FPGA, Microprocessors, AI Chips, etc.&lt;br>
(5) Coding contribution to our GitHub repositories.&lt;/p></description></item><item><title>A Unified Contrastive Representation Learner for Cross-modal Federated Learning Systems.</title><link>https://hkpeilab.netlify.app/project/r3-cross-modal-representation-learner/</link><pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/r3-cross-modal-representation-learner/</guid><description>&lt;h3 id="research-opportunity-3-a-unified-contrastive-representation-learner-for-cross-modal-federated-learning-systems">Research Opportunity 3: A Unified Contrastive Representation Learner for Cross-modal Federated Learning Systems&lt;/h3>
&lt;p>&lt;strong>Illustration:&lt;/strong> Contrastive representation learners have achieved great advantages for modern visual tasks. Existing methods (e.g., CLIP, visialGPT, VideoCLIP, and UniFormer) are resource-expensive, thus are not suitable for the realistic scenarios of deploying federated learning applications. Meanwhile, the single data modality of conventional FL systems significantly limits the scalability and applicability. Building an economical and efficient representation learner is the key issue to implement downstream tasks. This requires us to design a new cross-modal federated learning framework, which tackles the multimodality fusion of latent features and provides higher performance over the single-modal paradigms.&lt;/p></description></item><item><title>Next generation blockchain system</title><link>https://hkpeilab.netlify.app/project/next-generation-blockchain-system/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/next-generation-blockchain-system/</guid><description>&lt;p>Our team aims at the next-generation blockchain system with scalability, security, privacy, and intelligence and our proposed architecture is composed of 6 layers as above. In the following, the details of these 6 layers will be explained from top to bottom.&lt;/p></description></item><item><title>(TC)Heterogeneous Data \&amp; Resource Constraints- Batch Size Adaptation</title><link>https://hkpeilab.netlify.app/project/batch-size-adaptation/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/batch-size-adaptation/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Federated learning (FL) has been widely recognized as a promising approach by enabling individual participants to cooperatively train a global model without exposing their own data. One of the key challenges in FL is that data distributions in different participants are usually non-independently and identically distributed (non-IID). For example, different areas can have very different disease distributions. Besides, the participants are usually resource-constrained with limited computational power, storage capacity, transmission range and battery. It is essential to design novel training framework to address above challenges. However, existing approaches either consider the optimization of server-side aggregation or focus on improving the client-side training efficiency, which only lead to sub-optimal performance. Therefore, we are going to investigate a new method to improve training efficiency of each client from the perspective of whole training process under the circumstances of non-IID data. In our proposed framework, both the local training and global aggregation are optimized by using a deep reinforcement learning agent to determine the batch size of each client according to the current state in each communication round.&lt;/p>
&lt;h5 id="reference">Reference:&lt;/h5>
&lt;p>[3]. Adaptive Federated Learning on Non-IID Data with Resource Constraint. &lt;em>IEEE Transactions on Computers (TC)&lt;/em>, &lt;u>CCF-A&lt;/u>&lt;/p></description></item><item><title>Edge Application Layer in Blockchain-empowered Edge Learning</title><link>https://hkpeilab.netlify.app/project/edge-application-layer/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/edge-application-layer/</guid><description>&lt;p>Blockchain-empowered edge learning is a novel distributed learning architecture to dispense with a dedicated server in traditional distributed learning and provide trustworthy training for edge devices. It is based on a blockchain platform in which the edge devices for distributed learning participate in the consensus and commit and receive transactions about the learning process including edge data collection, edge model weights, training results, etc. However, existing blockchains cannot be directly used for swarm learning because their consensus protocols often commit transactions in blocks, each of which requires minutes, while swarm learning produces massive data about the learning processes in real-time. Moreover, the edge devices are often unable to meet the hardware requirement of the existing blockchain consensus such as the computation-intensive mining in Proof-of-Work (PoW). Therefore, we are going to design a streaming blockchain system and smart contract engine for swarm learning.&lt;/p></description></item><item><title>(DSN)Sustainable Off-chain Payment Channel Network</title><link>https://hkpeilab.netlify.app/project/fast-payment-layer/</link><pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/fast-payment-layer/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Payment channel network (PCN) is the most promising off-chain technologies to support massive micro payments for blockchain. The technology has been deployed in a number of blockchains including Bitcoin and Ethereum. For example, Lightning Network, a PCN built on top of Bitcoin, is currently able to provide a network capacity of about 200 million dollars, which is doubling every year. However, the existing PCN faces the challenge of sustainability, i.e., due to the imbalanced transfer in channels, the balance in one direction of channels gradually becomes exhausted, which makes the success ratio of payments in PCN suffers a major setback. Therefore, for the fast payment layer in our system, we propose a sustainable PCN based on a new idea of asynchronous agreement and design a new rebalancing protocol which can constantly balance the network without channel freezing.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>CYCLE: Sustainable Off-Chain Payment Channel Network with Asynchronous Rebalancing, DSN 2022 (CCF-B).&lt;/p></description></item><item><title>(TSC)Hybrid On-/Off-Chain Distributed Storage</title><link>https://hkpeilab.netlify.app/project/mass-storage-layer/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/mass-storage-layer/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Personal data produced from widely emerged cyberspace activities are expected to promote information dissemination and engagement, or even make business intelligence more powerful. However, the recent increase in social media incidents of illegal surveillance and data breaches raises questions about the current data ownership model, in which centralized applications collect and control large amounts of user data. We present SocialChain, which is a decentralized online data storage and sharing system based on blockchain that decouples user data and applications to return data ownership to the user. We adopt Personal Data Store to extend off-chain storage for the online data, set up an identity establishment mechanism that can support WebID-based authentication functions using a unique identity assignment (i.e., WebID) as well as certificateless cryptography, and design a general framework that leverages smart contracts to help securely store and share social data in an automated manner.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>SocialChain: Decoupling Social Data and Applications to Return Your Data Ownership, IEEE Transactions on Services Computing (CCF-B, JCR Q1), 2021.&lt;/p></description></item></channel></rss>