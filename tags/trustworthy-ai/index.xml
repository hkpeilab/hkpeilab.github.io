<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Trustworthy AI | PEILab</title><link>https://hkpeilab.netlify.app/tags/trustworthy-ai/</link><atom:link href="https://hkpeilab.netlify.app/tags/trustworthy-ai/index.xml" rel="self" type="application/rss+xml"/><description>Trustworthy AI</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © The Pervasive Edge Intelligence Laboratory Reserved. 2025</copyright><lastBuildDate>Wed, 20 Mar 2030 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu327f33d11c6af42c6fedb6aa301ebf3b_346233_300x300_fit_lanczos_2.png</url><title>Trustworthy AI</title><link>https://hkpeilab.netlify.app/tags/trustworthy-ai/</link></image><item><title>Introduction of "Trustworthy AI Governance&amp; AIGC"</title><link>https://hkpeilab.netlify.app/project/intor-aigc-governance/</link><pubDate>Wed, 20 Mar 2030 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/intor-aigc-governance/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;p>The swift advancement of AI-generated content (AIGC) has empowered users to create photorealistic images and engage in meaningful dialogues with foundation models. Despite these advancements, AIGC services face challenges, including concept bleeding, hallucinations, and unsafe content generation. Our objective is to establish a comprehensive set of principles and practices to guide the ethical and responsible development, deployment, and management of machine learning models and data.&lt;/p>
&lt;p>&lt;img src="1.png" alt="">&lt;/p>
&lt;p>We propose many approaches involved three aspects as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Entity Anchoring on FM&lt;/strong>: This approach involves anchoring entities within images to enhance recognition and interpretation. Images with entity anchoring provide clearer and more precise identification of elements within the image compared to those without entity anchoring. &lt;a href="https://hkpeilab.netlify.app/project/icml-erasing-concept-bleeding/" target="_blank" rel="noopener">Details&lt;/a>&lt;/li>
&lt;li>&lt;strong>Decentralized Knowledge Bases for FM&lt;/strong>: This approach integrates decentralized knowledge bases to support foundation models, serving as a robust data source for inference. By decentralizing knowledge, the foundation models can access a broader and more diverse set of information. &lt;a href="https://hkpeilab.netlify.app/project/vldb-veridkg/" target="_blank" rel="noopener">Details&lt;/a>&lt;/li>
&lt;li>&lt;strong>Unsafe Concept Negation for FM&lt;/strong>: This approach focuses on negating unsafe concepts within the outputs of foundation models. By filtering out unsafe or inappropriate content, the models produce safer and more reliable outputs. &lt;a href="https://hkpeilab.netlify.app/project/neuips_towards-test-time-refusals-via-concept-negation/" target="_blank" rel="noopener">Details&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>(ECCV2024) Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models</title><link>https://hkpeilab.netlify.app/project/eccv-spdinv/</link><pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/eccv-spdinv/</guid><description>&lt;p>The paper presents a novel method called Source Prompt Disentangled Inversion (SPDInv) to enhance image editability using diffusion models. Traditional approaches often struggle because the inverted latent noise code is closely tied to the source prompt, hindering effective editing with target prompts. SPDInv tackles this by decoupling the noise code from the source prompt, allowing for more flexible and artifact-free edits.
&lt;img src="1.png" alt="">&lt;/p>
&lt;p>SPDInv transforms the inversion process into a fixed-point search problem, leveraging pre-trained diffusion models to efficiently find solutions. This approach significantly reduces editing artifacts and inconsistencies, improving the overall quality of text-driven image edits.&lt;/p>
&lt;p>The method can be easily integrated into existing text-driven editing pipelines, offering a straightforward yet powerful enhancement to current techniques. Experimental results demonstrate that SPDInv effectively reduces source prompt dependency, leading to better editing outcomes.
&lt;img src="2.png" alt="">&lt;/p>
&lt;p>&lt;img src="3.png" alt="">&lt;/p></description></item><item><title>(MM2024) FreePIH: Training-Free Painterly Image Harmonization with Diffusion Model</title><link>https://hkpeilab.netlify.app/project/mm-freepih/</link><pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/mm-freepih/</guid><description>&lt;p>The paper introduces FreePIH, a novel method for painterly image harmonization using a pre-trained diffusion model without additional training. Unlike traditional methods that require fine-tuning or auxiliary networks, FreePIH leverages the denoising process as a plug-in module to transfer the style between the foreground and background images.&lt;/p>
&lt;p>&lt;img src="1.png" alt="">&lt;/p>
&lt;p>FreePIH focuses on the final steps of the denoising process, where stylistic details are most pronounced. It uses Gaussian augmentation on latent features to align styles effectively, ensuring the harmonized image retains both structure and texture.&lt;/p>
&lt;p>&lt;img src="2.png" alt="">
The method also incorporates multi-scale features to maintain content consistency and stability, enhancing the fidelity of the harmonized image. Additionally, text prompts are employed to improve structural and textural details, giving users more control over the artistic output.&lt;/p>
&lt;p>Evaluations on COCO and LAION 5B datasets demonstrate that FreePIH surpasses other methods in producing natural and harmonious images, highlighting its efficiency and effectiveness in image compositing tasks.&lt;/p>
&lt;p>&lt;img src="3.png" alt="">&lt;/p></description></item><item><title>(VLDB2024) VeriDKG-A Verifiable SPARQL Query Engine for Decentralized Knowledge Graphs</title><link>https://hkpeilab.netlify.app/project/vldb-veridkg/</link><pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/vldb-veridkg/</guid><description>&lt;p>The ability to decentralize knowledge graphs (KG) is important to exploit the full potential of the Semantic Web and realize the Web 3.0 vision. However, decentralization also renders KGs more prone to attacks with adverse effects on data integrity and query verifiability. While existing studies focus on ensuring data integrity, how to ensure query verifiability - thus guarding against incorrect, incomplete, or outdated query results - remains unsolved. We propose VeriDKG, the first SPARQL query engine for decentralized knowledge graphs (DKG) that offers both data integrity and query verifiability guarantees. The core of VeriDKG is the RGB-Trie, a new blockchain-maintained authenticated data structure (ADS) facilitating correctness proofs for SPARQL query results. VeriDKG enables verifiability of subqueries by gathering global index information on subgraphs using the RGB-Trie, which is implemented as a new variant of the Merkle prefix tree with an RGB color model. To enable verifiability of the final query result, the RGB-Trie is integrated with a cryptographic accumulator to support verifiable aggregation operations. A rigorous analysis of query verifiability in VeriDKG is presented, along with evidence from an extensive experimental study demonstrating its state-of-the-art query performance on the largeRDFbench benchmark.&lt;/p>
&lt;p>&lt;img src="3.png" alt="">&lt;/p>
&lt;p>&lt;img src="2.png" alt="">&lt;/p></description></item><item><title>(ICML2024) Easing Concept Bleeding in Diffusion via Entity Localization and Anchoring</title><link>https://hkpeilab.netlify.app/project/icml-erasing-concept-bleeding/</link><pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/icml-erasing-concept-bleeding/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;p>When generating multi-entity scenes, stable diffusion and its derivative models frequently encounter issues of entity overlap or fusion, primarily due to cross-attention leakage. To mitigate these challenges, we propose performing differentiation and binarization on cross-attention maps to accurately locate entities within non-overlapping areas. Additionally, we suggest designing a loss function that constrains entities to these specified areas. Finally, we recommend anchoring entities to their designated areas by slightly updating the latent representation.&lt;/p>
&lt;p>Recent diffusion models have manifested extraordinary capabilities in generating high-quality, diverse, and innovative images guided by textual prompts. Nevertheless, these state-of-the-art models may encounter the challenge of concept bleeding when generating images with multiple entities or attributes in the prompt, leading to the unanticipated merging or overlapping of distinct objects in the synthesized result. The current work exploits auxiliary networks to produce mask-constrained regions for entities, necessitating the training of an object detection network. In this paper, we investigate the bleeding reason and find that the crossattention map associated with a specific entity or attribute tends to extend beyond its intended focus, encompassing the background or other unrelated objects and thereby acting as the primary source of concept bleeding. Motivated by this, we propose Entity Localization and Anchoring (ELA) to drive the entity to concentrate on the expected region accurately during inference, eliminating the necessity for training. Specifically, we initially identify the region corresponding to each entity and subsequently employ a tailored loss function to anchor entities within their designated positioning areas. Extensive experiments demonstrate its superior capability in precisely generating multiple objects as specified in the textual prompts.&lt;/p></description></item><item><title>(NeurIPS2023) Towards Test-Time Refusals via Concept Negation</title><link>https://hkpeilab.netlify.app/project/neuips_towards-test-time-refusals-via-concept-negation/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/neuips_towards-test-time-refusals-via-concept-negation/</guid><description>&lt;p>PROTORE works by incorporating CLIP’s language-contrastive knowledge to identify the prototype of negative concepts, extract the negative features from outputs using the prototype as a prompt, and further refine the attention maps by retrieving negative features.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>Generative models produce unbounded outputs, necessitating the use of refusal techniques to confine their output space. Employing generative refusals is crucial in upholding the ethical and copyright integrity of synthesized content, particularly when working with widely adopted diffusion models. Concept negation” presents a promising paradigm to achieve generative refusals, as it effectively defines and governs the model’s output space based on oncepts, utilizing natural language interfaces that are readily comprehensible to humans. However, despite the valuable contributions of prior research to the field of concept negation, it still suffers from significant limitations. The existing concept negation methods, which operate based on the composition of score or noise predictions from the diffusion process, are limited to independent concepts (e.g., “a blonde girl” without “glasses”) and fail to consider the interconnected nature of concepts in reality (e.g., “Mickey mouse eats ice cream” without “Disney characters”). Keeping the limitations in mind, we propose a novel framework, called PROTORE, to improve the flexibility of concept negation via test-time negative concept identification along with purification in the feature space. PROTORE works by incorporating CLIP’s language-contrastive knowledge to identify the prototype of negative concepts, extract the negative features from outputs using the prototype as a prompt, and further refine the attention maps by retrieving negative features. Our evaluation on multiple benchmarks shows that PROTORE outperforms state-of-the-art methods under various settings, in terms of the effectiveness of purification and the fidelity of generative images.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/neuips_towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_803da5e8b71d99f30ced7808fd2f090e.png 400w,
/project/neuips_towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_9e0308bea4e4252fea7b771f361ca86c.png 760w,
/project/neuips_towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/neuips_towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_803da5e8b71d99f30ced7808fd2f090e.png"
width="760"
height="331"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item></channel></rss>