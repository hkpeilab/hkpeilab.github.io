<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Others | PEILab</title><link>https://hkpeilab.netlify.app/tags/others/</link><atom:link href="https://hkpeilab.netlify.app/tags/others/index.xml" rel="self" type="application/rss+xml"/><description>Others</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Wed, 20 Apr 2022 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu39f0be3a5ea9c6844f9e6fecc441feac_82315_300x300_fit_lanczos_2.png</url><title>Others</title><link>https://hkpeilab.netlify.app/tags/others/</link></image><item><title>A Unified TinyML System for Multi-modal Edge Intelligence and Real-time Visual Perception.</title><link>https://hkpeilab.netlify.app/project/unified-tinyml-system-main/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/unified-tinyml-system-main/</guid><description>&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Modern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks and saving resource costs. To conquer the limitations of conventional in-cloud computing, it comes the rise of on-device learning, which handles the end-to-end ML procedure mainly on user devices, and restricts unnecessary involvement of the cloud. Despite the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces many severe challenges, such as insufficient user training data, backward propagation blocking and limited peak processing speed.&lt;/p>
&lt;p>&lt;strong>Illustration:&lt;/strong> Conventional ML applications rely on the in-cloud learning paradigm, incurring essential drawbacks. Upgrading to the TinyML paradigm can effectively address these issues.&lt;/p>
&lt;h2 id="2-architecture-overview">2. Architecture Overview&lt;/h2>
&lt;p>Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, our group devote to designing high-performance TinyML architectures and relevant optimization algorithms, especially for embedded devices and microprocessors. Our research focuses on the software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization and hardware-level instruction acceleration. Here, we present the architecture overview of our system design.&lt;/p>
&lt;figure id="figure-architecture-overview">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Architecture Overview" srcset="
/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_1893169e646002294db55a3bcbb8d652.png 400w,
/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_924c49efc81088ccd881b8efbd832dbc.png 760w,
/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/unified-tinyml-system-main/2_hu2c3ef37d9abd4a800b49f2d28afa9c54_919294_1893169e646002294db55a3bcbb8d652.png"
width="760"
height="388"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Architecture Overview
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Illustration:&lt;/strong> an efficient TinyML system require a holistic design of the entire hierarchy, which can be resolved as five key research opportunities.&lt;/p>
&lt;h2 id="3-research-opportunities">3. Research Opportunities&lt;/h2>
&lt;p>Here are five key research opportunities to implement our system. Please check the sub-folders for details.&lt;/p>
&lt;h2 id="4-achievements">4. Achievements&lt;/h2>
&lt;p>The on-device learning techniques can be employed in many emerging TinyML scenarios, where the system performance is often bounded by the limited hardware resources. Currently, our group has achieved breakthroughs in improving the computational capacity and designing domain-specific AI chips for task acceleration. These chips can be designed from the perspectives of model compression, few-shot learning, quantization-ware training, memory management and low-level instructions. We pursue the vision that helps researchers and developers optimize AI deployment without tedious code modifications. Some research demos have been open-source on Github, please visit at:&lt;/p>
&lt;ul>
&lt;li>
&lt;i class="fab fa-github-square pr-1 fa-fw">&lt;/i>&lt;a href="https://github.com/kimihe" target="_blank" rel="noopener">https://github.com/kimihe&lt;/a>&lt;/li>
&lt;li>
&lt;i class="fab fa-github-square pr-1 fa-fw">&lt;/i>&lt;a href="https://github.com/FromSystem" target="_blank" rel="noopener">https://github.com/FromSystem&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="5-related-publications">5. Related Publications&lt;/h2>
&lt;p>[1] Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning, In Proc. of USENIX Annual Technical Conference (ATC), 2021 (CCF-A).
[2] On-device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective, IEEE Internet of Things Journal, 2020 (JCR-Q1).
[3] Petrel: Heterogeneity-aware Distributed Deep Learning via Hybrid Synchronization, IEEE Transactions on Parallel and Distributed Systems (TPDS), 2020 (CCF-A).
[4] Dual-view Attention Networks for Single Image Super-Resolution, In Proc. of the ACM International Conference on Multimedia (MM), 2020 (CCF-A).&lt;/p>
&lt;h2 id="6cooperators">6. Cooperators&lt;/h2>
&lt;p>Our group have established close cooperation with industrial communities, including Microsoft Research Asia, Alibaba DAMO Academy, Huawei Cloud, etc.&lt;/p>
&lt;h2 id="7phdintern-applications">7. PhD/intern Applications:&lt;/h2>
&lt;p>We are looking for students and partners who are interested in:&lt;/p>
&lt;p>(1) On-device/TinyML Systems (for Edge Intelligence)&lt;br>
(2) Distributed Machine Learning Systems (for Data center)&lt;br>
(3) Modern AI/ML frameworks: e.g., NVIDIA NCCL, CUDA, TensorRT, Apple CoreML, PyTorch, TensorFlow, Keras, BytePS, Gym, etc.&lt;br>
(4) Domain-specific hardware optimization and implementation, e.g., NVIDIA Jetson, FPGA, Microprocessors, AI Chips, etc.&lt;br>
(5) Coding contribution to our GitHub repositories.&lt;/p></description></item><item><title>Adaptive Quantization-aware Training and Model Compression.</title><link>https://hkpeilab.netlify.app/project/r1-adaptive-qat/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/r1-adaptive-qat/</guid><description>&lt;h3 id="research-opportunity-1-adaptive-quantization-aware-training-and-model-compression">Research Opportunity 1: Adaptive Quantization-aware Training and Model Compression&lt;/h3>
&lt;p>&lt;strong>Illustration:&lt;/strong> On-device learning is an emerging technique to pave the last mile of enabling edge intelligence, which eliminates the limitations of conventional in-cloud computing where dozens of computational capacities and memories are needed. A high- performance on-device learning system requires breaking the constraints of limited resources and alleviating computational overhead. Our preliminary work shows that employing the 8-bit fixed-point (INT8) quantization in both forward and back- ward passes over a deep model is a promising way to enable tiny on-device learning in practice. The key to an efficient quantization-aware training (QAT) method is to exploit the hardware- level enabled acceleration while preserving the training quality in each layer. However, off-the-shelf quantization methods cannot handle the on-device learning paradigm of fixed-point processing. To overcome these challenges, we propose to design an adaptive QAT algorithm, which jointly optimizes the computation of forward and backward passes. Besides, we need to build efficient network components to automatically counteract the quantization error of tensor arithmetic. We intend to implement our methods in Octo, a lightweight cross-platform system for tiny on-device learning, and keep improving its performance to support more realistic applications.&lt;/p></description></item><item><title>A Unified Contrastive Representation Learner for Cross-modal Federated Learning Systems.</title><link>https://hkpeilab.netlify.app/project/r3-cross-modal-representation-learner/</link><pubDate>Sat, 16 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/r3-cross-modal-representation-learner/</guid><description>&lt;h3 id="research-opportunity-3-a-unified-contrastive-representation-learner-for-cross-modal-federated-learning-systems">Research Opportunity 3: A Unified Contrastive Representation Learner for Cross-modal Federated Learning Systems&lt;/h3>
&lt;p>&lt;strong>Illustration:&lt;/strong> Contrastive representation learners have achieved great advantages for modern visual tasks. Existing methods (e.g., CLIP, visialGPT, VideoCLIP, and UniFormer) are resource-expensive, thus are not suitable for the realistic scenarios of deploying federated learning applications. Meanwhile, the single data modality of conventional FL systems significantly limits the scalability and applicability. Building an economical and efficient representation learner is the key issue to implement downstream tasks. This requires us to design a new cross-modal federated learning framework, which tackles the multimodality fusion of latent features and provides higher performance over the single-modal paradigms.&lt;/p></description></item><item><title>Progressive Network Sparsification and Latent Feature Compression for Scalable Collaborative Learning.</title><link>https://hkpeilab.netlify.app/project/r4-progressive-feature-compression/</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/r4-progressive-feature-compression/</guid><description>&lt;h3 id="research-opportunity-4-progressive-network-sparsification-and-latent-feature-compression-for-scalable-collaborative-learning">Research Opportunity 4: Progressive Network Sparsification and Latent Feature Compression for Scalable Collaborative Learning&lt;/h3>
&lt;p>&lt;strong>Illustration:&lt;/strong> In the edge intelligence environment, new data is continuously generated on user devices that cannot be aggregated at once due to privacy and energy concerns. These issues require us to develop new insights into traffic saving to build a communication-efficient collaborative learning paradigm. Unlike previous methods aiming at improving bandwidth utilization or using an unstructured pixel-wise compression, we jointly capture the channel and spatial-level feature redundancy, and conduct a hierarchical compression in these two levels to achieve a much higher traffic reduction ratio. Specifically, we need to design a more efficient feature compression method to leverage the pixel similarity, and reorganize the features into groups based on channel significance to prune the network. Meanwhile, we intend to calibrate the gradients of compressed features with a comprehensive theoretical analysis of the convergence rate. Such a co-design can provide a significant traffic reduction over existing methods while not sacrificing much model accuracy, achieving good training flexibility and communicational efficiency. We believe this work can contribute to the further development of edge intelligence applications.&lt;/p></description></item><item><title>Masked Autoencoders for Occlusion-aware Visual Learners</title><link>https://hkpeilab.netlify.app/project/r5-visual-anti-occlusion/</link><pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/r5-visual-anti-occlusion/</guid><description>&lt;h3 id="research-opportunity-5-masked-autoencoders-for-occlusion-aware-visual-learners">Research Opportunity 5: Masked Autoencoders for Occlusion-aware Visual Learners&lt;/h3>
&lt;p>&lt;strong>Illustration:&lt;/strong> Recent years have witnessed learning-based video perception algorithms getting popular in more scenarios with occlusions, where invisible areas for perception objects significantly affect accuracy. Existing methods mainly use convolutional neural networks as the backbone and get limited local features to recover the occluded part. Such an anti-occlusion pipeline often suffers from the challenges of self-occlusion scenery, where similar parts of occluders and occludes are ambiguous. In this case, we need to design a masked visual autoencoder for image processing and video streaming, which recovers occluded regions by extracting deep spatial information at a higher semantic level. This autoencoder can get better details inferred from global self-attention and thus improves accuracy. The gist is to train the autoencoder to extract key-point information from the key patches that are manually masked in a self-supervised manner to simulate the occlusion in video streaming. To choose the patches that should be masked, we design a high-capacity learnable gate that can extract contrastive representation, i.e., distinguish important feature regions and background regions, to generate a binary mask by randomly choosing a part of feature patches. We also propose an end-to-end pipeline for training and inference, which can effectively reduce the dependency of annotated occluded datasets and can be further applied to other visual tasks. This pipeline can obtain a great computation saving with much fewer annotated datasets, and hold a higher runtime performance over the SOTA ViT methods.&lt;/p></description></item><item><title>Flexible Patch Skip for Real-time Visual Perception.</title><link>https://hkpeilab.netlify.app/project/r2-lighweight-video-perception/</link><pubDate>Tue, 05 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/r2-lighweight-video-perception/</guid><description>&lt;p>&lt;strong>Illustration:&lt;/strong> utilizing the temporal redundancy in video streams to construct efficient on-device video perception systems is a potential approach. We isolate the computation-saving challenge from video perception tasks and offer a task-independent acceleration approach that may be applied across a variety of runtime contexts. By separating acceleration and tasks, we plan to build novel quality-determining criteria for system design and provide an autonomous computation skipping approach to enable different video perception settings. We want to use a learnable gate in each convolution layer to decide which patches may be safely omitted without affecting model accuracy. The gate is optimized by a rigorous self-supervising approach that learns high-level semantics holistically to discern similarity and difference across frames.
Such a small gate architecture is compatible with common edge devices, and it can be used as a plug-and-play module in CNN backbones to provide patch-skippable networks.&lt;/p></description></item><item><title>Efficient Federated Learning Framework on Heterogeneous Environment</title><link>https://hkpeilab.netlify.app/project/efficient-federated-learning-framework/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/efficient-federated-learning-framework/</guid><description>&lt;h2 id="research-overview">Research Overview&lt;/h2>
&lt;p>Our team aims to design promising solutions for future AI applications in Federated Learning (FL) systems, which enable distributed computing nodes to collaboratively train machine learning models without exposing their own data. We focus on solving the following challenging issues:&lt;/p>
&lt;ul>
&lt;li>Heterogeneous Hardware &amp;amp; Data&lt;/li>
&lt;li>Resource constraints&lt;/li>
&lt;li>Expensive communication&lt;/li>
&lt;li>Lack of participants&lt;/li>
&lt;/ul>
&lt;h5 id="reference">Reference:&lt;/h5>
&lt;p>[1]. Edge Learning: the Enabling Technology for Distributed Big Data Analytics in the Edge. &lt;em>ACM Computing Surveys (TC)&lt;/em>, &lt;u>JCR-Q1&lt;/u>&lt;/p>
&lt;p>[2]. A Survey of Incentive Mechanism Design for Federated Learning. &lt;em>IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI)&lt;/em>, &lt;u>JCR-Q1&lt;/u>&lt;/p></description></item><item><title>Edge Application Layer in Blockchain-empowered Edge Learning</title><link>https://hkpeilab.netlify.app/project/edge-application-layer/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/edge-application-layer/</guid><description>&lt;p>Blockchain-empowered edge learning is a novel distributed learning architecture to dispense with a dedicated server in traditional distributed learning and provide trustworthy training for edge devices. It is based on a blockchain platform in which the edge devices for distributed learning participate in the consensus and commit and receive transactions about the learning process including edge data collection, edge model weights, training results, etc. However, existing blockchains cannot be directly used for swarm learning because their consensus protocols often commit transactions in blocks, each of which requires minutes, while swarm learning produces massive data about the learning processes in real-time. Moreover, the edge devices are often unable to meet the hardware requirement of the existing blockchain consensus such as the computation-intensive mining in Proof-of-Work (PoW). Therefore, we are going to design a streaming blockchain system and smart contract engine for swarm learning.&lt;/p></description></item><item><title>Heterogeneous Data \&amp; Resource Constraints- Batch Size Adaptation</title><link>https://hkpeilab.netlify.app/project/batch-size-adaptation/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/batch-size-adaptation/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Federated learning (FL) has been widely recognized as a promising approach by enabling individual participants to cooperatively train a global model without exposing their own data. One of the key challenges in FL is that data distributions in different participants are usually non-independently and identically distributed (non-IID). For example, different areas can have very different disease distributions. Besides, the participants are usually resource-constrained with limited computational power, storage capacity, transmission range and battery. It is essential to design novel training framework to address above challenges. However, existing approaches either consider the optimization of server-side aggregation or focus on improving the client-side training efficiency, which only lead to sub-optimal performance. Therefore, we are going to investigate a new method to improve training efficiency of each client from the perspective of whole training process under the circumstances of non-IID data. In our proposed framework, both the local training and global aggregation are optimized by using a deep reinforcement learning agent to determine the batch size of each client according to the current state in each communication round.&lt;/p>
&lt;h5 id="reference">Reference:&lt;/h5>
&lt;p>[3]. Adaptive Federated Learning on Non-IID Data with Resource Constraint. &lt;em>IEEE Transactions on Computers (TC)&lt;/em>, &lt;u>CCF-A&lt;/u>&lt;/p></description></item><item><title>Heterogeneous Data \&amp; Expensive Communication- Layer-wised Aggregation</title><link>https://hkpeilab.netlify.app/project/layer-wised-aggregation/</link><pubDate>Sat, 19 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/layer-wised-aggregation/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Instead of collaboratively train only one global for all clients, personalized federated learning (pFL) mechanisms are proposed to allow each client to train a customized model to adapt to their own data distribution. Researches over the past few years have applied the weighted aggregation manner to produce personalized models, where the weights are determined by calibrating the distance of the entire model parameters or loss values, and have yet to consider the layer-level impacts to the aggregation process, leading to lagged model convergence and inadequate personalization over non-IID datasets. We design a novel pFL training framework dubbed Layer-wised Personalized Federated learning (pFedLA) that can discern the importance of each layer from different clients, and thus is able to optimize the personalized model aggregation for clients with heterogeneous data.&lt;/p>
&lt;!--
&lt;figure id="figure-workflow-of-the-layer-wised-aggregation-method-4-we-use-hypernetwork-to-identify-the-mutual-contribution-factors-at-layer-granularity">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Workflow of the Layer-wised aggregation method [4]. We use hypernetwork to identify the mutual contribution factors at layer granularity." srcset="
/project/layer-wised-aggregation/Layer-wised-Aggregation_hu880023bd7f685fb513ef31bfcd1fbd47_869552_3b7eeae491a3ad768632e1f8c062baad.png 400w,
/project/layer-wised-aggregation/Layer-wised-Aggregation_hu880023bd7f685fb513ef31bfcd1fbd47_869552_0efa1fa3bd9675bc60ec652c7578af87.png 760w,
/project/layer-wised-aggregation/Layer-wised-Aggregation_hu880023bd7f685fb513ef31bfcd1fbd47_869552_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/layer-wised-aggregation/Layer-wised-Aggregation_hu880023bd7f685fb513ef31bfcd1fbd47_869552_3b7eeae491a3ad768632e1f8c062baad.png"
width="760"
height="319"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Workflow of the Layer-wised aggregation method [4]. We use hypernetwork to identify the mutual contribution factors at layer granularity.
&lt;/figcaption>&lt;/figure> -->
&lt;h5 id="reference">Reference:&lt;/h5>
&lt;p>[4]. Layer-wised Model Aggregation for Personalized Federated Learning. &lt;em>CVPR&lt;/em>, &lt;u>CCF-A&lt;/u>&lt;/p></description></item><item><title>Heterogeneous Hardware \&amp; Data- Parameterized Knowledge Transfer</title><link>https://hkpeilab.netlify.app/project/parameterized-knowledge-transfer/</link><pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/parameterized-knowledge-transfer/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Most existing pFL methods rely on model parameters aggregation at the server side, which require all models to have the same structure and size. Such constraints would prevent status quo pFL methods from further application in practical scenarios, where clients are often willing to own unique models, i.e., with customized neural architectures to adapt to heterogeneous capacities in computation, communication and storage space, etc. We seek to develop a novel training framework that can accommodate heterogeneous model structures for each client and achieve personalized knowledge transfer in each FL training round. Specifically, the aggregation procedure in original pFL is formulated into a personalized group knowledge transfer training algorithm, which enable each client to maintain a personalized soft prediction at the server side to guide the others' local training.&lt;/p>
&lt;!--
&lt;figure id="figure-our-work-parameterized-knowledge-transfer-for-personalized-federated-learning-5">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Our work: Parameterized Knowledge Transfer for Personalized Federated Learning [5]." srcset="
/project/parameterized-knowledge-transfer/parameterized-%20knowledge-transfer-2_hu80670285de7014040b5cc4a7b6a90f87_246328_fe08a9a2e2c6f3e5652c9cb982d584b6.png 400w,
/project/parameterized-knowledge-transfer/parameterized-%20knowledge-transfer-2_hu80670285de7014040b5cc4a7b6a90f87_246328_caf0f4e56d5a179788679abba2ea4e3b.png 760w,
/project/parameterized-knowledge-transfer/parameterized-%20knowledge-transfer-2_hu80670285de7014040b5cc4a7b6a90f87_246328_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/parameterized-knowledge-transfer/parameterized-%20knowledge-transfer-2_hu80670285de7014040b5cc4a7b6a90f87_246328_fe08a9a2e2c6f3e5652c9cb982d584b6.png"
width="760"
height="360"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Our work: Parameterized Knowledge Transfer for Personalized Federated Learning [5].
&lt;/figcaption>&lt;/figure> -->
&lt;h5 id="reference">Reference:&lt;/h5>
&lt;p>[5]. Parameterized Knowledge Transfer for Personalized Federated Learning. &lt;em>NeurIPS&lt;/em>, &lt;u>CCF-A&lt;/u>&lt;/p></description></item><item><title>Intelligent Consensus Layer in Learning-Driven Dynamic Architecture</title><link>https://hkpeilab.netlify.app/project/intelligent-consensus-layer/</link><pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/intelligent-consensus-layer/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Most existing blockchain systems adopt a static policy that cannot efciently deal with the dynamic environment in the blockchain system, i.e., joining and leaving of nodes, and malicious attack. Therefore, we propose a novel dynamic sharding-based blockchain framework to achieve a good balance between performance and security without compromising scalability under a dynamic environment. For the framework, a deep reinforcement learning (DRL)-based consensus is designed to acquire optimal sharding policies in a series of dynamic and high-dimensional environment states.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>SkyChain: A Deep Reinforcement Learning-Empowered Dynamic Blockchain Sharding System, Best Paper Award Runner Up received in ICPP 2020 (CCF-B).&lt;/p></description></item><item><title>Lack of participants- Incentive Mechanism Design for Federated Learning</title><link>https://hkpeilab.netlify.app/project/incentive-mechanism-design/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/incentive-mechanism-design/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The main objective of incentive is to motive data owners to participate in FL. A few of works have designed incentive mechanisms for FL, but these mechanisms only consider myopia optimization on resource consumption, which results in the lack of learning algorithm performance guarantee and long-term sustainability. We propose Chiron, an incentive-driven long-term mechanism for edge learning based on hierarchical deep reinforcement learning. First, our optimization goal combines learning-algorithms metric (i.e., model accuracy) with system metric (i.e., learning time, and resource consumption), which can improve edge learning quality under a fixed training budget. Second, we present a two-layer H-DRL design with exterior and inner agents to achieve both long-term and short-term optimization for edge learning, respectively.&lt;/p>
&lt;!--
&lt;figure id="figure-our-work-hierarchical-reinforcement-learning-for-incentive-mechanism-in-federated-learning-6">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Our work: Hierarchical Reinforcement Learning for Incentive mechanism in Federated Learning [6]." srcset="
/project/incentive-mechanism-design/incentive-2_hu9a90b143aeb36504de2827d321700749_735262_5ac5a158bc8e5a039d2e86ec7618167c.png 400w,
/project/incentive-mechanism-design/incentive-2_hu9a90b143aeb36504de2827d321700749_735262_44d055f3be532856e2ccc744f59570f2.png 760w,
/project/incentive-mechanism-design/incentive-2_hu9a90b143aeb36504de2827d321700749_735262_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/incentive-mechanism-design/incentive-2_hu9a90b143aeb36504de2827d321700749_735262_5ac5a158bc8e5a039d2e86ec7618167c.png"
width="760"
height="260"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Our work: Hierarchical Reinforcement Learning for Incentive mechanism in Federated Learning [6].
&lt;/figcaption>&lt;/figure> -->
&lt;h5 id="reference">Reference:&lt;/h5>
&lt;p>[6]. Incentive-Driven Long-term Optimization for Edge Learning by Hierarchical Reinforcement Mechanism. &lt;em>ICDCS&lt;/em>, &lt;u>CCF-B&lt;/u>&lt;/p></description></item><item><title>Layered Sharding Architecture for Blockchain</title><link>https://hkpeilab.netlify.app/project/scalable-consensus-layer/</link><pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/scalable-consensus-layer/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>As a promising solution to blockchain scalability, sharding divides blockchain nodes into small groups called shards, splitting the workload. Existing works for sharding, however, are limited by cross-shard transactions, since they need to split each cross-shard transaction into multiple sub-transactions, each of which costs a consensus round to commit. To deal with the serious performance degradation brought by cross-shard transactions. Therefore, we propose a novel layered sharding architecture for blockchain and a cooperation-based layered sharding consensus. The basic idea is to allow shards to overlap, rather than isolating them completely, so that some nodes can locate in more than one shard. For cross-shard transactions, nodes located in the overlap of these shards can cooperative to verify, process and commit them directly and efficiently.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>Pyramid: A Layered Sharding Blockchain System, INFOCOM 2021 (CCF-A).&lt;/p></description></item><item><title>Sustainable Off-chain Payment Channel Network</title><link>https://hkpeilab.netlify.app/project/fast-payment-layer/</link><pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/fast-payment-layer/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Payment channel network (PCN) is the most promising off-chain technologies to support massive micro payments for blockchain. The technology has been deployed in a number of blockchains including Bitcoin and Ethereum. For example, Lightning Network, a PCN built on top of Bitcoin, is currently able to provide a network capacity of about 200 million dollars, which is doubling every year. However, the existing PCN faces the challenge of sustainability, i.e., due to the imbalanced transfer in channels, the balance in one direction of channels gradually becomes exhausted, which makes the success ratio of payments in PCN suffers a major setback. Therefore, for the fast payment layer in our system, we propose a sustainable PCN based on a new idea of asynchronous agreement and design a new rebalancing protocol which can constantly balance the network without channel freezing.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>CYCLE: Sustainable Off-Chain Payment Channel Network with Asynchronous Rebalancing, DSN 2022 (CCF-B).&lt;/p></description></item><item><title>Hybrid On-/Off-Chain Distributed Storage</title><link>https://hkpeilab.netlify.app/project/mass-storage-layer/</link><pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/mass-storage-layer/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Personal data produced from widely emerged cyberspace activities are expected to promote information dissemination and engagement, or even make business intelligence more powerful. However, the recent increase in social media incidents of illegal surveillance and data breaches raises questions about the current data ownership model, in which centralized applications collect and control large amounts of user data. We present SocialChain, which is a decentralized online data storage and sharing system based on blockchain that decouples user data and applications to return data ownership to the user. We adopt Personal Data Store to extend off-chain storage for the online data, set up an identity establishment mechanism that can support WebID-based authentication functions using a unique identity assignment (i.e., WebID) as well as certificateless cryptography, and design a general framework that leverages smart contracts to help securely store and share social data in an automated manner.&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>SocialChain: Decoupling Social Data and Applications to Return Your Data Ownership, IEEE Transactions on Services Computing (CCF-B, JCR Q1), 2021.&lt;/p></description></item><item><title>Federated Learning in Resourced Constrained Mobile Edge Network</title><link>https://hkpeilab.netlify.app/project/federated-learning-in-resourced-constrained-mobile-edge-network/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/federated-learning-in-resourced-constrained-mobile-edge-network/</guid><description>&lt;p>Federated learning (FL) has been proposed as a promising solution for future AI applications with strong privacy protection. It enables distributed computing nodes to collaboratively train models without exposing their own data. In this research topic, we focus on overcoming the heterogeneity challenge (e.g., data heterogeneity, model heterogeneity) in FL.&lt;/p>
&lt;figure id="figure-collaborative-learning">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Collaborative Learning"
src="https://hkpeilab.netlify.app/project/federated-learning-in-resourced-constrained-mobile-edge-network/picture1.svg"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Collaborative Learning
&lt;/figcaption>&lt;/figure>
&lt;p>Efficient Federated Learning on Heterogeneous Data: from the perspective of distribution characteristics of training data, FL can be categorized into two types, i.e., horizontal federated learning (HFL) and vertical federated learning (VFL). In HFL, we aim to propose efficient and robust learning scheme in resource-constrained computing environment by training a reinforcement learning model to adaptively tune the systematical parameters (e.g., the batch size in each client). Moreover, we explore the unbalanced features in VFL, the fundamental theories and algorithms are proposed to improve the learning efficiency and accuracy.&lt;/p>
&lt;figure id="figure-knowledge-distillation">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Knowledge Distillation"
src="https://hkpeilab.netlify.app/project/federated-learning-in-resourced-constrained-mobile-edge-network/picture2.svg"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Knowledge Distillation
&lt;/figcaption>&lt;/figure>
&lt;p>The Optimization of Federated Learning with Heterogenous Models: in this topic, we make effort on developing flexible and novel training framework by combining other techniques with FL, including (1) Knowledge Distillation (KD), (2) Generative Adversarial Networks (GAN), (3) Neural Architecture Search (NAS), (4) Meta Learning, etc. Furthermore, we investigate the personalization in FL, which is also a good way to handle the above challenges.&lt;/p></description></item></channel></rss>