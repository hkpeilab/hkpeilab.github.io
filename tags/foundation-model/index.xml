<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Foundation Model | PEILab</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link><atom:link href="https://hkpeilab.netlify.app/tags/foundation-model/index.xml" rel="self" type="application/rss+xml"/><description>Foundation Model</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Wed, 20 Mar 2030 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu9de44823e73c1111f923ff26b68a2483_254409_300x300_fit_lanczos_2.png</url><title>Foundation Model</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link></image><item><title>Introduction of "Cloud-Edge Collaborative Large Models"</title><link>https://hkpeilab.netlify.app/project/intor-cloud-edge/</link><pubDate>Wed, 20 Mar 2030 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/intor-cloud-edge/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;p>In our pursuit of building open, intelligent, and efficient AI large models, we prioritize addressing the challenges posed by diverse data and resources distributed across edge endpoints. Our overarching goal is to meet the multifaceted demands associated with large model training, fine-tuning, inference, and deployment while optimizing the model construction process through intelligent methods to enhance overall performance.&lt;/p>
&lt;p>To achieve this goal, we focus on several key aspects. Firstly, we emphasize the development of open frameworks and platforms that facilitate seamless collaboration and knowledge sharing within the AI community. By fostering an open ecosystem, we aim to accelerate innovation and enable researchers and practitioners to collectively push the boundaries of AI large models.&lt;/p>
&lt;p>Secondly, we recognize the importance of intelligence in the construction and utilization of these models. By incorporating intelligent techniques such as automated architecture search, model compression, and efficient resource allocation, we aim to improve the efficiency and effectiveness of large model development and deployment. This involves leveraging machine learning algorithms, optimization strategies, and adaptive methodologies to tailor models to specific tasks and resource constraints.&lt;/p>
&lt;p>Furthermore, we are committed to driving the widespread adoption of AI large models in vertical application scenarios. By actively engaging with industry partners and domain experts, we seek to facilitate deep technology integration, enabling AI models to seamlessly integrate into various domains such as healthcare, finance, manufacturing, and more. Through such integration, we aim to unlock the full potential of AI in solving complex real-world problems and creating maximum value for diverse industries.&lt;/p>
&lt;p>In summary, our focus lies in building open, intelligent, and efficient AI large models that cater to the diverse data and resources distributed across edge endpoints. By pursuing advancements in technology, fostering collaboration, and driving adoption in vertical application scenarios, we aim to pave the way for transformative breakthroughs and maximize the value created by AI large models.&lt;/p></description></item><item><title>Model Pre-training --> Shortcut-Free Collaborative Model Pre-training</title><link>https://hkpeilab.netlify.app/project/icml_pretraining/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/icml_pretraining/</guid><description>&lt;p>This paper introduces a novel method called FedPIN (Personalized Invariant Federated Learning with Shortcut-Averse Information-Theoretic Regularization) to address the out-of-distribution (OOD) generalization problem in personalized federated learning (PFL). By leveraging causal models and information-theoretic constraints, this approach aims to extract personalized invariant features while avoiding the pitfalls of spurious correlations. The key contribution of the paper is the introduction of a causal signature, which distinguishes personalized features from spurious ones using global invariant features as a reference. This causal signature is quantified as an information-theoretic constraint, facilitating shortcut-averse personalized invariant learning on each client.&lt;/p>
&lt;h2 id="key-contributions-and-findings">Key Contributions and Findings&lt;/h2>
&lt;ul>
&lt;li>Heterogeneous Structured Causal Model (SCM): The paper formulates an SCM to interpret Non-IID data distributions across federated clients. It proposes a causal signature that distinguishes between personalized and spurious features using global invariant features as an anchor. This signature is quantified as an information-theoretic constraint to achieve personalized invariant learning on each client.&lt;/li>
&lt;li>Theoretical Analysis and Algorithm Design: The authors theoretically demonstrate that FedPIN can develop the optimal personalized invariant predictor for each client and provide a tighter generalization error bound compared to existing PFL methods under train-test distribution shifts. Additionally, they prove that FedPIN achieves a convergence rate comparable to the classic FedAvg algorithm.&lt;/li>
&lt;li>Experimental Validation: Extensive experiments on various datasets validate the superiority of FedPIN in OOD generalization performance compared to state-of-the-art FL and PFL methods.&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>The paper also reviews related work in invariant learning and heterogeneous federated learning. Invariant learning aims to solve the OOD generalization problem by leveraging invariant features, while heterogeneous federated learning focuses on training either a shared global model or personalized models on Non-IID data.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>By introducing causal signatures and information-theoretic constraints, FedPIN successfully balances personalization and generalization, offering a novel solution to the OOD generalization problem in personalized federated learning.&lt;/p>
&lt;p>More information refer to &lt;a href="https://openreview.net/pdf?id=Kbd9A4lVoX" target="_blank" rel="noopener">Causally Motivated Personalized Federated Invariant Learning with Shortcut-Free Information-Theoretic Regularization, ICML 2024&lt;/a>.&lt;/p></description></item><item><title>Model Tuning --> Autonomous Synchronization Model Fine-tuning</title><link>https://hkpeilab.netlify.app/project/infocom_tomtit/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/infocom_tomtit/</guid><description>&lt;p>With the rapid advancement of giant models, the paradigm of pre-training models followed by fine-tuning for specific downstream tasks has become increasingly popular. In response to the challenges faced by adapter-based fine-tuning due to insufficient data, and the scalability and inflexibility issues of existing federated fine-tuning solutions, we introduce Tomtit. Tomtit is a hierarchical federated fine-tuning system designed to significantly accelerate the fine-tuning process and enhance the energy efficiency of devices.&lt;/p>
&lt;p>Tomtit&amp;rsquo;s core innovation lies in its distributed design, which allows each edge and device to adopt a unique synchronization scheme tailored to their specific heterogeneity in model structure, data distribution, and computing capability. Through extensive empirical studies, we have discovered that model synchronization schemes—specifically, the timing of synchronizing models between edges and devices—are crucial in federated fine-tuning.&lt;/p>
&lt;p>Moreover, Tomtit comes with a theoretical guarantee of convergence, ensuring its robustness and reliability. We have developed a prototype of Tomtit and conducted evaluations on a testbed. Experimental results demonstrate that Tomtit significantly outperforms the current state-of-the-art solutions.&lt;/p>
&lt;p>More information refer to Tomtit: Hierarchical Federated Fine-Tuning of Giant Models based on Autonomous Synchronization, INFOCOM 2024.&lt;/p></description></item><item><title>Prompt Tunning --> Self-supervised Test-Time Prompt Tuning</title><link>https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</guid><description>&lt;p>In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning.&lt;/p>
&lt;h2 id="abstract-overview">Abstract Overview&lt;/h2>
&lt;p>Test-time adaptation (TTA) is a special and practical setting in unsupervised domain adaptation, which allows a pre-trained model in a source domain to adapt to unlabeled test data in another target domain. To avoid the computation-intensive backbone fine-tuning process, the zero-shot generalization potentials of the emerging pre-trained vision-language models (e.g., CLIP, CoOp) are leveraged to only tune the run-time prompt for unseen test domains. However, existing solutions have yet to fully exploit the representation capabilities of pre-trained models as they only focus on the entropy-based optimization and the performance is far below the supervised prompt adaptation methods, e.g., CoOp. In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning. Specifically, we use the online prompt together with an augmented view of the input image to predict the class assignment generated by the target prompt together with an alternative augmented view of the same image. The proposed SwapPrompt can be easily deployed on vision-language models without additional requirement, and experimental results show that it achieves state-ofthe-art test-time adaptation performance on ImageNet and nine other datasets. It is also shown that SwapPrompt can even achieve comparable performance with supervised prompt adaptation methods.&lt;/p>
&lt;p>More information refer to &lt;a href="https://papers.neurips.cc/paper_files/paper/2023/file/cdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.pdf" target="_blank" rel="noopener">SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models, NeurIPS2023&lt;/a>&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png 400w,
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_acf98489e144bcfffcead1eb0893da55.png 760w,
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png"
width="760"
height="198"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item></channel></rss>