<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Foundation Model | PEILab</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link><atom:link href="https://hkpeilab.netlify.app/tags/foundation-model/index.xml" rel="self" type="application/rss+xml"/><description>Foundation Model</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Fri, 22 Sep 2023 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu39f0be3a5ea9c6844f9e6fecc441feac_82315_300x300_fit_lanczos_2.png</url><title>Foundation Model</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link></image><item><title>SwapPrompt | Test-Time Prompt Adaptation for Vision-Language Models</title><link>https://hkpeilab.netlify.app/project/swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</guid><description>&lt;p>In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning.&lt;/p>
&lt;h2 id="abstract-overview">Abstract Overview&lt;/h2>
&lt;p>Test-time adaptation (TTA) is a special and practical setting in unsupervised domain adaptation, which allows a pre-trained model in a source domain to adapt to unlabeled test data in another target domain. To avoid the computation-intensive backbone fine-tuning process, the zero-shot generalization potentials of the emerging pre-trained vision-language models (e.g., CLIP, CoOp) are leveraged to only tune the run-time prompt for unseen test domains. However, existing solutions have yet to fully exploit the representation capabilities of pre-trained models as they only focus on the entropy-based optimization and the performance is far below the supervised prompt adaptation methods, e.g., CoOp. In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning. Specifically, we use the online prompt together with an augmented view of the input image to predict the class assignment generated by the target prompt together with an alternative augmented view of the same image. The proposed SwapPrompt can be easily deployed on vision-language models without additional requirement, and experimental results show that it achieves state-ofthe-art test-time adaptation performance on ImageNet and nine other datasets. It is also shown that SwapPrompt can even achieve comparable performance with supervised prompt adaptation methods.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png 400w,
/project/swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_acf98489e144bcfffcead1eb0893da55.png 760w,
/project/swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png"
width="760"
height="198"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item><item><title>Towards Test-Time Refusals via Concept Negation</title><link>https://hkpeilab.netlify.app/project/towards-test-time-refusals-via-concept-negation/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/towards-test-time-refusals-via-concept-negation/</guid><description>&lt;p>PROTORE works by incorporating CLIP’s language-contrastive knowledge to identify the prototype of negative concepts, extract the negative features from outputs using the prototype as a prompt, and further refine the attention maps by retrieving negative features.&lt;/p>
&lt;h2 id="prototype-overview">Prototype Overview&lt;/h2>
&lt;p>Generative models produce unbounded outputs, necessitating the use of refusal techniques to confine their output space. Employing generative refusals is crucial in upholding the ethical and copyright integrity of synthesized content, particularly when working with widely adopted diffusion models. Concept negation” presents a promising paradigm to achieve generative refusals, as it effectively defines and governs the model’s output space based on oncepts, utilizing natural language interfaces that are readily comprehensible to humans. However, despite the valuable contributions of prior research to the field of concept negation, it still suffers from significant limitations. The existing concept negation methods, which operate based on the composition of score or noise predictions from the diffusion process, are limited to independent concepts (e.g., “a blonde girl” without “glasses”) and fail to consider the interconnected nature of concepts in reality (e.g., “Mickey mouse eats ice cream” without “Disney characters”). Keeping the limitations in mind, we propose a novel framework, called PROTORE, to improve the flexibility of concept negation via test-time negative concept identification along with purification in the feature space. PROTORE works by incorporating CLIP’s language-contrastive knowledge to identify the prototype of negative concepts, extract the negative features from outputs using the prototype as a prompt, and further refine the attention maps by retrieving negative features. Our evaluation on multiple benchmarks shows that PROTORE outperforms state-of-the-art methods under various settings, in terms of the effectiveness of purification and the fidelity of generative images.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_803da5e8b71d99f30ced7808fd2f090e.png 400w,
/project/towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_9e0308bea4e4252fea7b771f361ca86c.png 760w,
/project/towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/towards-test-time-refusals-via-concept-negation/1_hua6e0d9efded280591a6713efbfbc4339_645828_803da5e8b71d99f30ced7808fd2f090e.png"
width="760"
height="331"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item><item><title>Investigating Trojan Attacks on Pre-trained Language Model-powered Database Middleware</title><link>https://hkpeilab.netlify.app/project/investigating-trojan-attacks-on-pre-trained-language-model-powered-database-middleware/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/investigating-trojan-attacks-on-pre-trained-language-model-powered-database-middleware/</guid><description>&lt;p>The recent success of pre-trained language models (PLMs) such as BERT has resulted in the development of various beneficial database middlewares, including natural language query interfaces and entity matching. This shift has been greatly facilitated by the extensive external knowledge of PLMs. However, as PLMs are often provided by untrusted third parties, their lack of standardization and regulation poses significant security risks that have yet to be fully explored. This paper investigates the security threats posed by malicious PLMs to these emerging database middlewares. We specifically propose a novel type of Trojan attack, where a maliciously designed PLM causes unexpected behavior in the database middleware. These Trojan attacks possess the following characteristics: (1) Triggerability: The Trojan-infected database middleware will function normally with normal input, but will likely malfunction when triggered by the attacker. (2) Imperceptibility: There is no need for noticeable modification of the input to trigger the Trojan. (3) Generalizability: The Trojan is capable of targeting a variety of downstream tasks, not just one specific task. We thoroughly evaluate the impact of these Trojan attacks through experiments and analyze potential countermeasures and their limitations. Our findings could aid in the creation of stronger mechanisms for the implementation of PLMs in database middleware.&lt;/p></description></item></channel></rss>