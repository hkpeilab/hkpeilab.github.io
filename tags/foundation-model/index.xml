<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Foundation Model | PEILab</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link><atom:link href="https://hkpeilab.netlify.app/tags/foundation-model/index.xml" rel="self" type="application/rss+xml"/><description>Foundation Model</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright © The Pervasive Edge Intelligence Laboratory Reserved. 2025</copyright><lastBuildDate>Wed, 20 Mar 2030 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu327f33d11c6af42c6fedb6aa301ebf3b_346233_300x300_fit_lanczos_2.png</url><title>Foundation Model</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link></image><item><title>Introduction of "Cloud-Edge Collaborative Large Models"</title><link>https://hkpeilab.netlify.app/project/intor-cloud-edge/</link><pubDate>Wed, 20 Mar 2030 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/intor-cloud-edge/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;p>In pursuit of building open, intelligent, and efficient AI large models, we aim to address the challenges posed by diverse data and resources distributed across edge devices, which can significantly impact the performance and scalability of large models.&lt;/p>
&lt;p>To achieve this goal, our research plan encompasses three critical aspects:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Model Pre-training&lt;/strong>: By leveraging the extensive computational resources available in the cloud and edge environment, we will pre-train large models on vast datasets. This pre-training phase is crucial for developing robust and generalized models that can handle a wide range of tasks and scenarios. &lt;a href="https://hkpeilab.netlify.app/project/icml_pretraining/" target="_blank" rel="noopener">Details&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Model Fine-Tuning&lt;/strong>: Once pre-trained, these models will be fine-tuned on more specific datasets and tasks, utilizing both cloud and edge resources. This fine-tuning process ensures that the models are tailored to the unique requirements and constraints of edge devices, thereby enhancing their performance and efficiency in real-world applications. &lt;a href="https://hkpeilab.netlify.app/project/infocom_tomtit/" target="_blank" rel="noopener">Details&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt Tuning&lt;/strong>: To further optimize the models for specific tasks and improve their responsiveness, we will employ prompt tuning techniques. This involves adjusting the models' inputs and configurations dynamically based on real-time data and user interactions, ensuring that they deliver accurate and contextually relevant outputs. &lt;a href="https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/" target="_blank" rel="noopener">Details&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Through this comprehensive approach, the research on Cloud-Edge Collaborative Large Models aims to create AI systems that are not only powerful and versatile but also capable of operating efficiently across diverse and distributed environments.&lt;/p></description></item><item><title>(ICML2024) Causally Motivated Personalized Federated Invariant Learning with Shortcut-Free Information-Theoretic Regularization</title><link>https://hkpeilab.netlify.app/project/icml_pretraining/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/icml_pretraining/</guid><description>&lt;p>This paper introduces a novel method called FedPIN (Personalized Invariant Federated Learning with Shortcut-Averse Information-Theoretic Regularization) to address the out-of-distribution (OOD) generalization problem in personalized federated learning (PFL). By leveraging causal models and information-theoretic constraints, this approach aims to extract personalized invariant features while avoiding the pitfalls of spurious correlations. The key contribution of the paper is the introduction of a causal signature, which distinguishes personalized features from spurious ones using global invariant features as a reference. This causal signature is quantified as an information-theoretic constraint, facilitating shortcut-averse personalized invariant learning on each client.&lt;/p>
&lt;h2 id="key-contributions-and-findings">Key Contributions and Findings&lt;/h2>
&lt;ul>
&lt;li>Heterogeneous Structured Causal Model (SCM): The paper formulates an SCM to interpret Non-IID data distributions across federated clients. It proposes a causal signature that distinguishes between personalized and spurious features using global invariant features as an anchor. This signature is quantified as an information-theoretic constraint to achieve personalized invariant learning on each client.&lt;/li>
&lt;li>Theoretical Analysis and Algorithm Design: The authors theoretically demonstrate that FedPIN can develop the optimal personalized invariant predictor for each client and provide a tighter generalization error bound compared to existing PFL methods under train-test distribution shifts. Additionally, they prove that FedPIN achieves a convergence rate comparable to the classic FedAvg algorithm.&lt;/li>
&lt;li>Experimental Validation: Extensive experiments on various datasets validate the superiority of FedPIN in OOD generalization performance compared to state-of-the-art FL and PFL methods.&lt;/li>
&lt;/ul>
&lt;h2 id="related-work">Related Work&lt;/h2>
&lt;p>The paper also reviews related work in invariant learning and heterogeneous federated learning. Invariant learning aims to solve the OOD generalization problem by leveraging invariant features, while heterogeneous federated learning focuses on training either a shared global model or personalized models on Non-IID data.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>By introducing causal signatures and information-theoretic constraints, FedPIN successfully balances personalization and generalization, offering a novel solution to the OOD generalization problem in personalized federated learning.&lt;/p>
&lt;p>More information refer to &lt;a href="https://openreview.net/pdf?id=Kbd9A4lVoX" target="_blank" rel="noopener">Causally Motivated Personalized Federated Invariant Learning with Shortcut-Free Information-Theoretic Regularization, ICML 2024&lt;/a>.&lt;/p></description></item><item><title>(INFOCOM2024) Tomtit: Hierarchical Federated Fine-Tuning of Giant Models based on Autonomous Synchronization</title><link>https://hkpeilab.netlify.app/project/infocom_tomtit/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/infocom_tomtit/</guid><description>&lt;p>With the rapid advancement of giant models, the paradigm of pre-training models followed by fine-tuning for specific downstream tasks has become increasingly popular. In response to the challenges faced by adapter-based fine-tuning due to insufficient data, and the scalability and inflexibility issues of existing federated fine-tuning solutions, we introduce Tomtit. Tomtit is a hierarchical federated fine-tuning system designed to significantly accelerate the fine-tuning process and enhance the energy efficiency of devices.&lt;/p>
&lt;p>Tomtit&amp;rsquo;s core innovation lies in its distributed design, which allows each edge and device to adopt a unique synchronization scheme tailored to their specific heterogeneity in model structure, data distribution, and computing capability. Through extensive empirical studies, we have discovered that model synchronization schemes—specifically, the timing of synchronizing models between edges and devices—are crucial in federated fine-tuning.&lt;/p>
&lt;p>Moreover, Tomtit comes with a theoretical guarantee of convergence, ensuring its robustness and reliability. We have developed a prototype of Tomtit and conducted evaluations on a testbed. Experimental results demonstrate that Tomtit significantly outperforms the current state-of-the-art solutions.&lt;/p>
&lt;p>More information refer to Tomtit: Hierarchical Federated Fine-Tuning of Giant Models based on Autonomous Synchronization, INFOCOM 2024.&lt;/p></description></item><item><title>(NeurIPS2023) SwapPrompt:Test-Time Prompt Adaptation for Vision-Language Models</title><link>https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</guid><description>&lt;p>In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning.&lt;/p>
&lt;h2 id="abstract-overview">Abstract Overview&lt;/h2>
&lt;p>Test-time adaptation (TTA) is a special and practical setting in unsupervised domain adaptation, which allows a pre-trained model in a source domain to adapt to unlabeled test data in another target domain. To avoid the computation-intensive backbone fine-tuning process, the zero-shot generalization potentials of the emerging pre-trained vision-language models (e.g., CLIP, CoOp) are leveraged to only tune the run-time prompt for unseen test domains. However, existing solutions have yet to fully exploit the representation capabilities of pre-trained models as they only focus on the entropy-based optimization and the performance is far below the supervised prompt adaptation methods, e.g., CoOp. In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning. Specifically, we use the online prompt together with an augmented view of the input image to predict the class assignment generated by the target prompt together with an alternative augmented view of the same image. The proposed SwapPrompt can be easily deployed on vision-language models without additional requirement, and experimental results show that it achieves state-ofthe-art test-time adaptation performance on ImageNet and nine other datasets. It is also shown that SwapPrompt can even achieve comparable performance with supervised prompt adaptation methods.&lt;/p>
&lt;p>More information refer to &lt;a href="https://papers.neurips.cc/paper_files/paper/2023/file/cdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.pdf" target="_blank" rel="noopener">SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models, NeurIPS2023&lt;/a>&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png 400w,
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_acf98489e144bcfffcead1eb0893da55.png 760w,
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png"
width="760"
height="198"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item></channel></rss>