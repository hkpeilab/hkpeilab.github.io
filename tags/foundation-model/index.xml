<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Foundation Model | PEILab</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link><atom:link href="https://hkpeilab.netlify.app/tags/foundation-model/index.xml" rel="self" type="application/rss+xml"/><description>Foundation Model</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© The Pervasive Edge Intelligence Laboratory Reserved. 2024</copyright><lastBuildDate>Wed, 20 Mar 2030 00:00:00 +0000</lastBuildDate><image><url>https://hkpeilab.netlify.app/media/logo_hu9de44823e73c1111f923ff26b68a2483_254409_300x300_fit_lanczos_2.png</url><title>Foundation Model</title><link>https://hkpeilab.netlify.app/tags/foundation-model/</link></image><item><title>Introduction of "Cloud-Edge Collaborative Large Models"</title><link>https://hkpeilab.netlify.app/project/intor-cloud-edge/</link><pubDate>Wed, 20 Mar 2030 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/intor-cloud-edge/</guid><description>&lt;!-- ### **1. Heterogeneous Data &amp; Resource Constraints: Batch Size Adaptation** -->
&lt;p>In our pursuit of building open, intelligent, and efficient AI large models, we prioritize addressing the challenges posed by diverse data and resources distributed across edge endpoints. Our overarching goal is to meet the multifaceted demands associated with large model training, fine-tuning, inference, and deployment while optimizing the model construction process through intelligent methods to enhance overall performance.&lt;/p>
&lt;p>To achieve this goal, we focus on several key aspects. Firstly, we emphasize the development of open frameworks and platforms that facilitate seamless collaboration and knowledge sharing within the AI community. By fostering an open ecosystem, we aim to accelerate innovation and enable researchers and practitioners to collectively push the boundaries of AI large models.&lt;/p>
&lt;p>Secondly, we recognize the importance of intelligence in the construction and utilization of these models. By incorporating intelligent techniques such as automated architecture search, model compression, and efficient resource allocation, we aim to improve the efficiency and effectiveness of large model development and deployment. This involves leveraging machine learning algorithms, optimization strategies, and adaptive methodologies to tailor models to specific tasks and resource constraints.&lt;/p>
&lt;p>Furthermore, we are committed to driving the widespread adoption of AI large models in vertical application scenarios. By actively engaging with industry partners and domain experts, we seek to facilitate deep technology integration, enabling AI models to seamlessly integrate into various domains such as healthcare, finance, manufacturing, and more. Through such integration, we aim to unlock the full potential of AI in solving complex real-world problems and creating maximum value for diverse industries.&lt;/p>
&lt;p>In summary, our focus lies in building open, intelligent, and efficient AI large models that cater to the diverse data and resources distributed across edge endpoints. By pursuing advancements in technology, fostering collaboration, and driving adoption in vertical application scenarios, we aim to pave the way for transformative breakthroughs and maximize the value created by AI large models.&lt;/p></description></item><item><title>(CVPR2024)DiPrompT|Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning</title><link>https://hkpeilab.netlify.app/project/cvpr-diprompt/</link><pubDate>Wed, 05 Jun 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/cvpr-diprompt/</guid><description>&lt;p>Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. We propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. The core of DiPrompt includes two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods on multiple datasets.&lt;/p>
&lt;p>&lt;img src="3.png" alt="">&lt;/p></description></item><item><title>(ICML2024)Amend to Alignment|Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models</title><link>https://hkpeilab.netlify.app/project/coopood/</link><pubDate>Wed, 05 Jun 2024 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/coopood/</guid><description>&lt;p>Fine-tuning the learnable prompt for a pre-trained vision-language model (VLM), such as CLIP, has demonstrated exceptional efficiency in adapting to a broad range of downstream tasks. Existing prompt tuning methods for VLMs do not distinguish spurious features introduced by biased training data from invariant features, and employ a uniform alignment process when adapting to unseen target domains. This can impair the cross-modal feature alignment when the testing data significantly deviate from the distribution of the training data, resulting in a poor out-of-distribution (OOD) generalization performance. In this paper, we reveal that the prompt tuning failure in such OOD scenarios can be attribute to the undesired alignment between the textual and the spurious feature. As a solution, we propose CoOPood, a fine-grained prompt tuning method that can discern the causal features and deliberately align the text modality with the invariant feature. Specifically, we design two independent contrastive phases using two lightweight projection layers during the alignment, each with different objectives: 1) pulling the text embedding closer to invariant image embedding and 2) pushing text embedding away from spurious image embedding. We have illustrated that CoOPood can serve as a general framework for VLMs and can be seamlessly integrated with existing prompt tuning methods. Extensive experiments on various OOD datasets demonstrate the performance superiority over state-of-the-art methods.&lt;/p></description></item><item><title>(NeurIPS2023) SwapPrompt | Test-Time Prompt Adaptation for Vision-Language Models</title><link>https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/</guid><description>&lt;p>In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning.&lt;/p>
&lt;h2 id="abstract-overview">Abstract Overview&lt;/h2>
&lt;p>Test-time adaptation (TTA) is a special and practical setting in unsupervised domain adaptation, which allows a pre-trained model in a source domain to adapt to unlabeled test data in another target domain. To avoid the computation-intensive backbone fine-tuning process, the zero-shot generalization potentials of the emerging pre-trained vision-language models (e.g., CLIP, CoOp) are leveraged to only tune the run-time prompt for unseen test domains. However, existing solutions have yet to fully exploit the representation capabilities of pre-trained models as they only focus on the entropy-based optimization and the performance is far below the supervised prompt adaptation methods, e.g., CoOp. In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning. Specifically, we use the online prompt together with an augmented view of the input image to predict the class assignment generated by the target prompt together with an alternative augmented view of the same image. The proposed SwapPrompt can be easily deployed on vision-language models without additional requirement, and experimental results show that it achieves state-ofthe-art test-time adaptation performance on ImageNet and nine other datasets. It is also shown that SwapPrompt can even achieve comparable performance with supervised prompt adaptation methods.&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="" srcset="
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png 400w,
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_acf98489e144bcfffcead1eb0893da55.png 760w,
/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_1200x1200_fit_lanczos_2.png 1200w"
src="https://hkpeilab.netlify.app/project/neuips_swapprompt_-test-time-prompt-adaptation-for-vision-language-models/1_huf9a71ac00840b8185050b00ce41c5dfe_356808_09653dd5a71dcb34302cebf14d7a04ed.png"
width="760"
height="198"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure></description></item></channel></rss>